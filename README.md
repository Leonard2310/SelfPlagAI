# SelfPlagAI

The rise of **Large Language Models (LLMs)** has transformed natural language processing, enabling high-quality text generation at scale. As these models are increasingly used to produce publicly available content, a key issue emerges: **what happens when LLMs are trained on data largely generated by other LLMs?** This recursive use of synthetic data introduces the risk of **model collapse**, a phenomenon where models progressively lose alignment with real human language.

**SelfPlagAI** systematically investigates this effect on **question-answering tasks**. By simulating multiple generations of recursive fine-tuning‚Äîeach cycle using AI-generated outputs as training data‚Äîthe project measures how output quality degrades over time in terms of coherence, factual accuracy, and linguistic diversity. The aim is to provide concrete evidence of performance decay and to highlight the risks of uncontrolled self-learning loops in large-scale generative models.

<img src="https://github.com/user-attachments/assets/e2093380-2f08-4c22-95ac-9ead605a5f06" width="150"/>

Model collapse occurs when generative models, repeatedly trained on their own outputs, drift away from the true distribution of human language. Early stages lose rare but informative patterns, while later stages produce **homogenized, low-variance outputs**. This gradual degradation stems from accumulating statistical, expressive, and optimization-related errors across generations, ultimately threatening the sustainability and reliability of future LLMs.


## Requirements

* **MongoDB Community Server**
  Install from the official website:
  [https://www.mongodb.com/try/download/community](https://www.mongodb.com/try/download/community)

**Python 3.10+**
  All dependencies are listed in `requirements.txt`. Install via:
  `pip install -r requirements.txt`


## Architecture & Module Overview

<img src="https://github.com/user-attachments/assets/2736f6c3-ee4c-4f2f-b956-ce12abaa1c9d" width="500"/>


SelfPlagAI is organized into several modular components, each responsible for a specific phase of the recursive training pipeline:

1. **`db_utils.py`**
   Provides MongoDB utilities for connection, insertion, retrieval, updating, and cleanup of collections. Handles conversion between pandas DataFrames and MongoDB documents, with support for nested arrays and metadata stamping.

2. **`train_utils.py`**

   * **Data Preparation & Custom Trainer**: Reads SQuAD splits from MongoDB, formats prompts, tokenizes examples, and sets up a bespoke Trainer that combines standard language modeling loss with a BERTScore-based objective and an early-stopping callback.
   * **Synthetic Data Generation**: Uses the fine-tuned model to produce new answers, validates them against context, tracks generation success rates, and packages outputs into HuggingFace Datasets.
   * **Iterative Training Loop**: Automates multi-generation workflows, handling model loading (base or previous checkpoint), LoRA-based parameter-efficient fine-tuning, checkpoint management, and memory cleanup between iterations.
   * **Model Evaluation**: Generates predictions on a held-out test set, computes a suite of metrics (Exact Match, token-level F1, BERTScore F1, Jaccard semantic similarity), and saves both a detailed text report and structured JSON/CSV summaries.
   * **Prediction Export**: Exports model outputs to JSON files and optionally to MongoDB, enabling downstream analysis or visualization.

<img src="https://github.com/user-attachments/assets/da6b7578-205f-4055-bf42-d519a8bd8758" width="500"/>


3. **`selfTrain.py`**
   Orchestrates the end-to-end recursive fine-tuning process: it extracts a subset of SQuAD v2 examples, loops through successive training cycles, generates synthetic question-answer pairs using the fine-tuned model, evaluates model performance after each cycle, and exports both data and metadata back to MongoDB.

4. **`load_result_to_mongo.py`**
   Implements command-line tools to ingest JSON log files‚Äîboth evaluation results and prediction exports‚Äîinto MongoDB. Automatically tags each record with source file and directory metadata, and offers flags to clear existing collections before loading.


## Setup & Usage

1. **Configure environment variables**
   Create a `.env` file with your MongoDB credentials (`MONGO_USERNAME`, `MONGO_PASSWORD`).

2. **Load data into MongoDB**
   Use the loader script to ingest raw SQuAD v2 JSON files or previous evaluation logs into your database.

3. **Run recursive training**
   Execute `selfTrain.py` with command-line options to specify database names, collection names, number of generations, and whether to start from a pre-fine-tuned checkpoint. The script will:

   * Sample a controlled subset of SQuAD examples.
   * Fine-tune the model for each generation.
   * Generate synthetic training data and save it back to MongoDB.

4. **Evaluate each generation**
   Invoke the evaluation functions to compute and save metrics on the held-out test set. Evaluation artifacts include per-generation JSON results and a comparative CSV summarizing performance trends.

5. **Export predictions**
   Optionally, run the export utilities to dump all model predictions (contexts, questions, references, raw outputs) into JSON files and MongoDB collections for further analysis or for powering a front-end.


## Frontend Dashboard (Streamlit)

A user-friendly Streamlit app provides interactive visualization and exploration of SelfPlagAI‚Äôs results. It consists of two main modules:

1. **`main.py`**

   * **Dashboard Layout**: A wide-layout page titled ‚Äúüß† Dashboard SelfPlagAI‚Äù with three control panels:

     * **Model selector** to choose among base model names.
     * **Database selector** to pick the MongoDB evaluation collection.
     * **View mode selector** (‚ÄúGenerations‚Äù vs. ‚ÄúQuestion‚Äù) to switch between aggregate trends and per-question analysis.
   * **Generations View**: Plots line charts of evaluation metrics (Exact Match, F1, BERTScore F1, Semantic Similarity) across all generations for the selected model/database pair.
   * **Question View**:

     * Presents a dropdown of all test-set questions.
     * Displays a multi-metric line chart showing how the selected question‚Äôs performance evolves across generations.
     * Shows the question‚Äôs context and reference answer, alongside a table of the model‚Äôs prediction at each generation.

2. **`utils/mongo.py`**

   * **Data Retrieval**: Connects to MongoDB using credentials from `key.env` and the shared `db_utils` module.
   * **Metric Aggregation**: Reads the `evaluation_results` collection into a pandas DataFrame, extracts per-generation dictionaries of scores and metadata, and constructs a unified table with one row per generation containing all aggregated and individual-example metrics.

A simplified alternative UI lives in **`metriche.py`**, offering the same two view modes with built-in Streamlit line charts and dataframes for quick prototyping.

With this backend-to-frontend pipeline, SelfPlagAI equips researchers and engineers to:

* **Monitor model quality** over recursive training cycles.
* **Investigate individual examples**, spotting where self-training helps or fails.
* **Compare multiple metrics** side by side, supporting data-driven decisions on filtering or data-provenance controls.

Proceed to launch the dashboard:

```bash
cd streamlit_app
streamlit run main.py
```


## Authors

* [Emanuele Cuono Amoruso](https://github.com/KaminariManu)
* [Leonardo Catello](https://github.com/Leonard2310)
* [Lorenzo Manco](https://github.com/Rasbon99)
* [Carmine Grosso](https://github.com/httpix3l)

---

## License

This project is licensed under the [MIT License](LICENSE). For full terms, see the LICENSE file.
